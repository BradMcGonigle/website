---
title: AI-Assisted iOS Development in Practice
description: A day in the life of building an iOS app with an AI coding assistant—from Apple Reminders integration to live widget debugging at 9pm on a Saturday.
date: 2025-02-14
createdAt: 2025-02-14T00:00:00.000Z
draft: true
tags:
  - iOS
  - AI
  - Swift
  - Development
  - OpenClaw
---

I spent today building features for Park Pulse, a native iOS app I'm working on for Walt Disney World wait times. But I wasn't coding alone—I had an AI assistant helping me through the entire process, powered by [OpenClaw](https://openclaw.ai). Here's what that workflow actually looks like.

## What is OpenClaw?

OpenClaw is an AI agent framework that connects Claude (Anthropic's AI) to your local development environment. Unlike browser-based AI assistants, OpenClaw runs as a daemon on your machine with direct access to your filesystem, terminal, and tools. It can read and write files, run shell commands, query APIs, and—crucially for iOS development—build and deploy apps directly to your devices.

The magic is in the integration layer. OpenClaw connects to messaging platforms (I use iMessage), so I can text my assistant from anywhere. It reads my Apple Reminders to pull task lists. It knows my device IDs for deployment. It maintains persistent memory across sessions so context accumulates over time.

Think of it as giving Claude hands and eyes on your development machine, with a communication channel that works from your pocket.

## The Setup

Park Pulse is a SwiftUI app targeting iOS 17+, built with zero third-party dependencies. It uses WidgetKit for home screen widgets, App Intents for Siri shortcuts, and talks to the ThemeParks.wiki API for live data. The codebase has a comprehensive `CLAUDE.md` file that documents architecture decisions, coding standards, and patterns—essentially a rulebook that both I and the AI follow.

My task list lives in Apple Reminders, in a list called "Park Pulse - Ready to Implement." When I'm ready to work, I message OpenClaw to check that list and start implementing.

## The Workflow

Here's how today actually went:

**Morning:** I asked the assistant to pull my reminders and start on the first item—adding deep linking to the dining widget. It read the existing widget code, understood the pattern from the wait time widget, and implemented the feature. Build, deploy to my iPhone, test. Worked first try.

**Afternoon:** Interactive itinerary widget with a "mark complete" button. This one was trickier—widgets can't have arbitrary interactivity, they need App Intents. The assistant figured out the pattern, implemented it, and we iterated on the UX. I wanted a 10-second "undo" window instead of instant completion. The assistant understood why (fat fingers happen) and implemented a pending state with timestamps.

**Evening:** I'm at home, checking my widgets, and notice Casey's Corner (a quick service hot dog spot) shows "—" instead of "Open." I message the assistant: "Why are some quick service dining locations not showing Open?"

Within minutes, it:
1. Queried the live API to check what data Casey's Corner actually returns
2. Discovered the API simply doesn't track quick service restaurants—no live data entry exists
3. Found the code path that defaulted to "UNKNOWN" when no data was present
4. Fixed it to assume "OPERATING" when the park is open and no explicit status exists
5. Built and deployed to my phone

I tested, confirmed it worked, and told it to commit and push. Done.

## What Makes This Work

A few things make this flow effective:

**Context is king.** The `CLAUDE.md` file in my repo is ~500 lines of architectural decisions, code patterns, and conventions. The assistant reads it and follows the rules. When it adds a new feature, it follows the existing patterns. When I say "add a dashboard card," it knows exactly what that means and where the code goes.

**Direct device deployment.** This is where OpenClaw really shines for iOS development. The assistant knows my iPhone's device ID and can run the full Xcode build pipeline followed by `xcrun devicectl` to deploy directly to my phone. I describe a bug, OpenClaw writes the fix, builds the app, and pushes it to my device—all while I'm sitting on my couch. No need to be at my Mac or have Xcode open. The tight feedback loop this enables is transformative.

**Domain knowledge accumulates.** The assistant remembers that I'm a Disney annual passholder, that I prefer casual communication, that I want to test before committing. It doesn't ask clarifying questions it already knows the answers to.

**I stay in the loop.** The assistant doesn't commit until I approve. It doesn't push until I say so. For anything external-facing (messages, emails, public posts), it asks first. This isn't about distrust—it's about maintaining ownership of the final product.

## The Casey's Corner Debugging Session

Let me zoom in on that evening debugging session, because it illustrates the power of this setup.

It's 9pm on a Saturday. I'm on my couch, checking my widgets, and notice Casey's Corner shows "—" instead of "Open." I pull out my phone and send an iMessage: "Why are some quick service dining locations not showing Open?"

OpenClaw, running on my Mac in my office, receives the message and gets to work. It queries the live ThemeParks API, discovers that quick service restaurants don't have live data entries, traces through the widget code to find where the status defaults to "UNKNOWN", and implements a fix to assume "OPERATING" when the park is open.

Then it runs `xcodebuild`, waits for the compile to finish, and deploys the updated app directly to my iPhone using `xcrun devicectl`. A few minutes after sending that text, I have a fresh build on my phone to test.

**Traditional debugging flow:** Notice bug → Walk to computer → Open Xcode → Find relevant code → Understand the data flow → Query the API manually → Form a hypothesis → Write a fix → Build → Test → Iterate.

**OpenClaw flow:** Notice bug → Send a text → Test the fix on my phone → Reply "commit and push."

I never left the couch. The assistant handled all the mechanical work—navigating the codebase, querying external APIs, building, deploying. I just described the symptom and validated the solution.

## What Still Requires Human Judgment

Despite the efficiency, plenty of decisions still need me:

- **UX decisions:** The assistant can implement any interaction pattern I describe. Choosing the right one—like the 10-second undo window—requires understanding how people actually use the app.
- **Priority calls:** Which feature matters most? When is something good enough to ship? These require product sense, not code sense.
- **Architecture trade-offs:** When the assistant suggests an approach, I still evaluate whether it fits the system's future needs, not just today's requirements.
- **Quality bar:** The assistant follows the patterns in `CLAUDE.md`, but I set that bar. If I accept sloppy code, I get more sloppy code.

## The Meta-Point

This workflow embodies what I wrote about in [What LLMs Won't Replace](/blog/what-llms-wont-replace). The AI accelerates implementation dramatically, but the surrounding skills—understanding requirements, making trade-offs, maintaining quality—remain firmly human responsibilities.

The difference is that with faster implementation, I can iterate more. I can try an idea, see it on my phone, and decide whether it's right. That rapid feedback loop improves the final product more than any individual feature would.

## Try It Yourself

If you're interested in this workflow, [OpenClaw](https://openclaw.ai) is open source and available on GitHub. Here's what I'd suggest to get started:

1. **Set up the communication layer.** OpenClaw supports iMessage, Signal, Telegram, Discord, and more. Being able to message your assistant from your phone changes the workflow entirely.
2. **Document your architecture.** A `CLAUDE.md` or `AGENTS.md` file that explains your patterns, conventions, and decisions. The AI will follow them.
3. **Configure device deployment.** For iOS, this means having your device ID ready and ensuring your Mac can build and deploy. OpenClaw handles the rest.
4. **Stay in the approval loop.** Let the AI draft, but you decide what ships. OpenClaw waits for explicit approval before commits and external actions.
5. **Describe problems, not solutions.** "Casey's Corner shows wrong status" is better than "change line 47 to return OPERATING." The AI might find a better fix than you imagined.

This isn't the future of development. It's just today, on a Saturday, building an app I care about with an AI that can actually touch my development environment. But it's a pretty good today.
